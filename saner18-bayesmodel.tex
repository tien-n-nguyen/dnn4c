%\section{Bayesian Inference-based LM}

%\subsection{Basic model}
%
%In our model, we want to predict the next lexeme based on the syntactic, semantic and lexical contexts in which the prediction is performed.
%\begin{equation}
%\begin{aligned}
	%&p(lex_n|Syn, Sem, Lex) \\
	%=&p(Syn, Sem, Lex, lex_n)/p(Syn, Sem, Lex) \\
	%\propto& p(Syn, Sem, Lex, lex_n)
%\end{aligned}
%\end{equation}
%
%where:
%
 %$lex_n$: lexeme at location n (next lexeme)
%
%$p(Syn, Sem, Lex, lex_n)$: probabilities that syntactic, semantic, lexical contexts and the next lexeme co-appears
%
 %$Syn$: syntactic context
%
 %$Sem$: semantic context
%
 %$Lex$: lexical context
%
%$p(Syn, Sem, Lex)$ can be discarded (given the current context).
%
%\subsection{Model with  Conditional Dependency Assumptions}
%
%
%
%\subsubsection{All Independence Assumption}
%
%The simplest model assumes that syntactic, semantic and lexical contexts are all conditional independent given the next lexeme.
%\begin{equation}
%\label{indeq}
%\begin{aligned}
%&p(lex_n|Syn, Sem, Lex)  \\
%\propto&p(Syn, Sem, Lex, lex_n) = p(Syn, Sem, Lex|lex_n)p(lex_n)\\
%=&p(Syn|lex_n)p(Sem|lex_n)p(Lex|lex_n)p(lex_n)
%\end{aligned}
%\end{equation}
%
%Each component of the formula can be calculated by:
%\[
%\begin{aligned}
%&p(Syn|lex_n)= p(lex_n|Syn)p(Syn)/p(lex_n)\propto p(lex_n|Syn)/p(lex_n) \\
%&p(Sem|lex_n)= p(lex_n|Sem)p(Sem)/p(lex_n)\propto p(lex_n|Sem)/p(lex_n) \\
%&p(Lex|lex_n)= p(lex_n|Lex)p(Lex)/p(lex_n)\propto p(lex_n|Lex)/p(lex_n)
%\end{aligned}
%\]
%
%Then, equation {\ref{indeq}} can be rewritten as
%
%\begin{equation}
%\label{reindeq}
%\begin{aligned}
%p(lex_n|Syn, Sem, Lex)
%\propto& p(Syn|lex_n)p(Sem|lex_n)p(Lex|lex_n)p(lex_n) \\
%\propto&  \frac{p(lex_n|Syn)}{p(lex_n)} \frac{p(lex_n|Sem)}{p(lex_n)} \frac{p(lex_n|Lex)}{p(lex_n)} p(lex_n) \\
%=& p(lex_n|Syn) p(lex_n|Sem) p(lex_n|Lex) / p^2(lex_n)
%\end{aligned}
%\end{equation}
%
%The formulas to estimate probabilities in equation {\ref{reindeq}} are
%
%\vspace{6pt}
%1. $p(lex_n) = \#lex_n/\#all\_lexs$
%where $\#lex_n$ is the number of occurrences of $lex_n$ in all documents and $\#all\_lexs$ is total number of occurrences of all lexemes in all documents.
%
%\vspace{6pt}
%2. To deal with lexical context, we use the n-gram model, which n-1 previous lexemes compose the context for next lexeme prediction
%
%$p(lex_n|Lex) = p(lex_n|lex_1..lex_{n-1}) = (\#lex_1..lex_{n-1}lex_n)/(\#lex_1..lex_{n-1})$
%
%where $\#lex_1..lex_{n-1}$ is the number of occurrences of n-1 gram of previous lexemes and $\#lex_1..lex_{n-1}lex_n$ is the number of co-occurrences of that n-1 gram and then $lex_n$. $lex_i$ is label of the lexeme at location $i$.
%
%\vspace{6pt}
%3. We use the same strategy to deal with syntactic context
%
%$p(lex_n|Syn) = p(lex_n|syn_1..syn_{n-1}) = (\#syn_1..syn_{n-1}lex_n)/(\#syn_1..syn_{n-1})$
%
%where $\#syn_1..syn_{n-1}$ is the number of occurrences of n-1 gram of syntactic roles of previous lexemes and $\#syn_1..syn_{n-1}lex_n$ is the number of co-occurrences of that n-1 gram and then $lex_n$. $syn_i$ is the syntactic role of the lexeme at location $i$.
%
%\vspace{6pt}
%4. We use the same strategy to deal with semantic context
%
%$p(lex_n|Sem) = p(lex_n|sem_1..sem_{n-1}) = (\#sem_1..sem_{n-1}lex_n)/(\#sem_1..sem_{n-1})$
%
%where $\#sem_1..sem_{n-1}$ is the number of occurrences of n-1 gram of semantic annotation of previous lexemes and $\#sem_1..sem_{n-1}lex_n$ is the number of co-occurrences of that n-1 gram and then $lex_n$. $sem_i$ is the semantic annotation of the lexeme at location $i$.
%
%
%\subsubsection{Dependency with Hidden Factor}






%This approach is different with above approach in the way it assumes
%there is a hidden factor affects to different layers. The distribution
%of hidden factor is predefined and the parameters of that distribution
%is learned.

%N-gram model can be extended to a bayesian generative model which consider the context of the prediction process via the document topic. The document topic in turn is reflected via syntactic, semantic and lexical sequences.

%N-gram topic model (\cite{}) is an wellknown model used to deal with order of tokens and the topic of document. Although the standard model cannot deal with syntactic and semantic sequences, we can modified the model with little effort to include those features.

%\textbf{Gibbs sampling}. Gibbs sampling is an well-established technique to train the generative models like topic models.

%We can assume that syntaxemes and sememes and previous lexemes are generated by a hidden factor and previous lexemes and the hidden factor have causal relation with current lexeme, i.e.

%\subsubsection{Comparison of Training Objective with Bayesian-based LM}

%\begin{figure}[t]
%\centering
%\includegraphics[width=0.46\textwidth]{bayes.pdf}
%\caption{Bayesian-based LM with hidden factor}
%\label{bayesfig}
%\end{figure}

\subsection{Comparison with Bayesian-based LM in Code Completion}

%\subsubsection{Comparison of Training Objective with Bayesian-based LM}

%\input{tableBLM}

In {\tool}, we incorporate syntactic and semantic features of the
contexts 
%by adapting Huang's model~\cite{huang12} into 
via our context-aware DNN-based model
%our DNN-based training objective method
(Fig.~\ref{contextfig}).
% Section~\ref{dnnlmmodelsec}). 
In this experiment, we aimed to compare our {\em DNN-based
context-incorporating method} via training objective to {\em the
Bayesian inference-based incorporating method}, which was used in the
existing work SLAMC~\cite{fse13}. In our previous experiment, we
compared {\tool} and SLAMC. But that experiment did not achieve our
above goal since SLAMC uses only lexemes and sememes, and does not
include syntaxemes. To compare our DNN-based feature incorporating
method and the Bayesian-based one in SLAMC, we added sememes
into SLAMC to make it into a new Bayesian-based LM (denoted by {\em
BLM}), that incorporates all three types of features (lexemes,
syntaxemes, and sememes) as used in {\tool}.
% but with Bayesian Inference as in SLAMC~\cite{fse13}.
%
%
%Let us explain the key extensions in BLM and then our comparing BLM
%with {\tool}.
%We then compared the accuracy of BLM and {\tool}.
SLAMC combines the features in lexemes and sememes using Bayesian
inference-based $n$-gram topic model. The idea is that
the probability that a token $c$ appears is estimated based on the
prior $n$-1 lexical tokens $Lex$ and their sememes $Sem$, as well as
the topic $k$ of the token in the current file in which topics as
a hidden factor have the causal relations with $Lex$ and $Sem$. We
used the same computing mechanism of SLAMC but incorporated the
syntaxemes for the syntactic context to estimate the probability that
$c$ appears: $P(c|Lex,Syn,Sem)$. The computation of such
probability in training and predicting uses Bayesian
inference with $n$-gram topic model in the same way as in
SLAMC~\cite{fse13}.
 
%We compared the accuracy of BLM and {\tool}.

%In SLAMC, the probability that a token $c$ appears is estimated based
%on the prior $n$-1 lexical tokens and sememes, as well as the topic of
%the tokens in the current file. For a comparison between the
%incorporating mechanisms in {\tool} (DNN-based
%training objective) and SLAMC (Bayesian Inference), we incorporated
%syntaxemes into SLAMC as follows.
%%In BLM, such probability is estimated in the same way except that it
%%is also based on the syntaxeme and sememe sequences representing the
%%contexts.
%In SLAMC, the topics as a hidden factor have causal relations with
%sequences of lexemes and sememes. In BLM, they also have such
%relations with the sequences of syntaxemes.
%
%Similar to SLAMC, BLM assumes the current project has $K$ topics
%corresponding to its technical functionality. Each file has the topics
%with different percentages, which is represented by a hidden parameter
%called {\em topic proportion} $\theta$.
%$\theta$ is a multinomial distribution sampled from the Dirichlet
%distribution Dir($\alpha$, $K$).
%In SLAMC, the generating probability of a code token $c$ is dependent
%on its topic assignment $k$ as well as on its local context $lex$ and
%sequence $sem$ of sememes. In BLM, we extended that the probability
%generating token $c$ is dependent on its assigned topic $k$ and the
%history lexical sequence $lex$, syntaxeme sequence $syn$, and sememe
%sequence $sem$ ($P(c|Lex,Syn,Sem)$). 
%The computation for such probabilities and distributions is based on
%Bayesian inference as in SLAMC. Details on training and predicting are
%the same as in SLAMC~\cite{fse13}.

%===================================================================
%\subsubsection{Comparison of Training Objective with Bayesian-based LM}

%%In {\tool}, we incorporate syntactic and semantic contexts via our
%%DNN-based training objective method (Figure~\ref{contextfig},
%%Section~\ref{dnnlmmodelsec}). In our next experiment, we aimed to
%%compare our {\em DNN-based context-incorporating method} via
%%training objective to {\em the incorporating method via Bayesian
%%inference} (which was used in the existing work
%%SLAMC~\cite{fse13}). However, SLAMC does not include syntaxemes. For a
%%fair comparison, we extended SLAMC into a new Bayesian-based LM
%%(denoted by {\em BLM}), that considers both syntactic and semantic
%%contexts via Bayesian inference. Let us explain the key extensions in
%%BLM and then our comparing it with {\tool}.
%%
%%In SLAMC, the probability that a token $c$ appears is estimated based
%%on the prior $n$-1 lexical tokens and sememes, as well as the topic of
%%the tokens in the current file. For a comparison between the
%%incorporating mechanisms in {\tool} (DNN-based
%%training objective) and SLAMC (Bayesian Inference), we incorporated
%%syntaxemes into SLAMC as follows.
%%%In BLM, such probability is estimated in the same way except that it
%%%is also based on the syntaxeme and sememe sequences representing the
%%%contexts.
%%In SLAMC, the topics as a hidden factor have causal relations with
%%sequences of lexemes and sememes. In BLM, they also have such
%%relations with the sequences of syntaxemes.
%%%
%%Similar to SLAMC, BLM assumes the current project has $K$ topics
%%corresponding to its technical functionality. Each file has the topics
%%with different percentages, which is represented by a hidden parameter
%%called {\em topic proportion} $\theta$.
%%%$\theta$ is a multinomial distribution sampled from the Dirichlet
%%%distribution Dir($\alpha$, $K$).
%%In SLAMC, the generating probability of a code token $c$ is dependent
%%on its topic assignment $k$ as well as on its local context $lex$ and
%%sequence $sem$ of sememes. In BLM, we extended that the probability
%%generating token $c$ is dependent on its assigned topic $k$ and the
%%history lexical sequence $lex$, syntaxeme sequence $syn$, and sememe
%%sequence $sem$ ($P(c|Lex,Syn,Sem)$). The computation for such
%%probabilities and distributions is based on Bayesian inference as in
%%SLAMC. Details on training and predicting are the same as in
%%SLAMC~\cite{fse13}.

%===================================================================
%With the $n$-gram topic modeling in SLAMC, the probability that a
%token $c$ appears is estimated based on the prior $n$-1 lexical tokens
%as well as the topic of the tokens in the current file.  In BLM, such
%probability is estimated simultaneously based on $n$-1 prior lexical
%tokens as well as the syntactic and semantic sequences representing
%the contexts. Such contexts are driven by the ``topics'' of the
%current file through a generative process.  Figure {\ref{bayesfig}}
%shows the graph model for the causal relations between the topics as a
%hidden factor and the value of lexical, syntactic and semantic
%tokens. BLM assumes the current project has $K$ topics corresponding
%to its technical functionality. Each file has the topics with
%different percentages, which is represented by a hidden parameter
%called {\em topic proportion} $\theta$. $\theta$ is a multinomial
%distribution sampled from the Dirichlet distribution Dir($\alpha$,
%$K$). In SLAMC, the generating probability of a code token $c$ is
%dependent on its topic assignment $k$ as well as on its local context
%$l$, which is modeled by a multinomial distribution $\phi_{k,l}$
%(called token distribution).
%%, (also a sample of the Dirichlet distribution Dir($\beta$, $V$)).
%The training algorithm in SLAMC takes the a codebase, pre-defined
%hyper-parameters of Direchlet distributions, the maximum value of $n$,
%and outputs token distributions $\phi_{k,l}$ for every topic $k$ and
%all possible $n$-grams $l$.  After training, the model also assigns
%topic porportion for each file and the topic assignment for every
%position in the file.
%
%In BLM, we extend that the probability generating token $c$ is
%dependent on its topic assignment $k$ and the history lexical sequence
%$Lex$, syntaxeme sequence $syn$, and sememe sequence $sem$. We want to
%estimate the generating probability for any available token $lex_n$
%and sequence $Lex$, $Syn$, and $Sem$, of the last $n$-1 tokens.
%The token distribution is $\phi_{k,Lex,Syn,Sem}$.
%The Bayesian formula is:
%\begin{equation}
%\label{bayeseq}
%\begin{aligned}
%p(Syn, Sem, lex_1,..lex_{n-1}, lex_n, \theta)  \\
%= p(lex_n|lex_1,.., lex_{n-1},\theta) \times p(lex_1,.., lex_{n-1}|\theta)\\
%\times p(Syn|\theta)\times p(Sem|\theta) \times p(\theta)
%\end{aligned}
%\end{equation}
%
%We need to compute the prior distributions and likelihoods of
%$p(\theta), p(Syn|\theta), p(Sem|\theta), p(lex_1,..,
%lex_{n-1}|\theta)$ and $p(lex_n|lex_1,.., lex_{n-1}, \theta)$.
%
%The generative model for a source code file $d$ is:
%
%1. Sample $\theta^d$ from a Dirichlet prior with hyper parameter
%$\alpha$
%
%For each location $i$ in sequence
%
%2. Draw $z_i$ from $\theta^d$
%
%3. Draw $syn_i$ with Multinomial distribution of $z_i$
%
%3. Draw $sem_i$ with Multinomial distribution of $z_i$
%
%4. Draw each lexical token $lex_i$ from the distribution for the context defined by the topic $z_i$ and previous words
%   $lex_{i-n},..,lex_{i−1}$, according to product of distributions $Multinomial(z_i)$ and $P(lex_i|lex_{i-n},..,lex_{i−1})$
%
%The process of training and predicting is similar to that of $n$-gram
%topic model in SLAMC~\cite{fse13} except the following:
%
%\noindent {\bf Training.} First, we use the existing topic assignments
%of all sequences (variables $z$) (or randomly initiates for the first
%iteration) to estimate the token distributions $\phi_{k,Lex,Syn,Sem}(lex_n)$
%for every possible topic $k$ and $n$-grams. The estimation is done by
%taking the ratio between the number the sequences at three levels of
%size $n$ having the topic $k$ over the total number of sequences
%of the topic without requiring $lex_n$ at the end.
%
%We then use the estimated token distributions $\phi_{k,Lex,Syn,Sem}$
%to compute the topic assignment $z$ for every sequence of size $n$ at
%three levels. Once topics are assigned for all positions (i.e. $z_i$
%is sampled for every $i$), the topic proportion $\theta$ is
%re-estimated as the number of tokens on each topic over the number of
%tokens. The sampling and re-estimating is repeated on each sequence
%until $\theta$ and $\phi$ is stable for all sequences or the maximum
%number of iterations is reached.

%\noindent {\bf Prediction.} The prediction algorithm takes as input
%the trained values in $\phi_{k,Lex,Syn,Sem}$ (token distributions for
%all topics and all $n$-grams of lexemes, syntaxemes, and sememes.  For
%a sequence $Lex$, it builds $Syn$ and $Sem$.  It first estimates the
%topic proportion $\theta$. Then, it estimates the generating
%probability $P(c|Lex,Syn,Sem)$ for any available token $c$ and the
%sequences of the last $n-1$ tokens of $Lex$, $Syn$, and $Sem$.

%We checked the reason that {\textbf{\tool}} and
%{\textbf{\tool}*} have much improvement at high $top-k$. We saw that
%{\textbf{N-gram}}, {\textbf{SLAMC}} and {\textbf{SLAMC+Syn}} cannot
%predict $n-gram$ if that $n-gram$ has never appeared which leads to
%the $top-k$ accuracy get flat at high $k$. In other way,
%{\textbf{\tool}*} and {\textbf{\tool}} can predict event $n-gram$
%which has never appeared due to the mechanism of similar context in
%DNN language model.

\input{tableBLM}

\vspace{0.03in}
\noindent {\bf Results.} As seen in Table~\ref{tab:blm},
% displays the result. As seen, 
{\tool} relatively outperforms BLM up to {\bf 19.1\%}
%from {\bf 3.3--19.1\%} 
at top-1 accuracy and up to {\bf 47.6\%}
%from {\bf 5.1--47.6\%} 
at top-5 accuracy. At the higher top ranks, the~gap between
{\tool} and BLM is even larger (not shown). This result shows that
using the same feature set,
%of features of lexemes, syntaxemes, and sememes,
the DNN-based context-incorporating method
%with training objective 
(Fig.~\ref{contextfig}) enables {\tool} to perform better
than the Bayesian-based LM with feature incorporating method using
Bayesian Inference 
%via $n$-gram topic modeling 
as in SLAMC~\cite{fse13}.

Let us compare the accuracy of BLM in Table~\ref{tab:blm} and that of
SLAMC in Table~\ref{allaccuracytab}. Note that BLM is a new model that
just adds into SLAMC a new feature type of syntaxemes for syntactic
context. Specifically, BLM with syntaxemes has a relative improvement
over SLAMC up to {\bf 7.8\%}
%from {\bf 3.6--7.8\%} 
for top-1 accuracy. The relative improvement is up to 4.0\%
%from 1.5--4.0\% 
for top-5 accuracy, and gets larger for higher top ranks (not
shown). This result shows that syntactic context via the newly
introduced feature type, syntaxemes, is really useful.
% in improving
% next-token suggestion accuracy in SLAMC.

%both the models.
