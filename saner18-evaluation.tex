\section{Empirical Evaluation}

%talk about other apps?

This section presents our empirical evaluation. In our experiments,
we aim 1) to evaluate {\tool}'s {\em accuracy} in two SE
applications: code completion and code migration; 2)
to compare it with the state-of-the-art approaches; and 3) to study
the impacts on accuracy of different {\em parameters} of the model and
those of {\em syntactic and semantic contexts}.

%next-token suggestion; 2) to compare {\tool} to the state-of-the-art
%LMs; 3) to study the impacts on accuracy of different {\em parameters}
%of the model and those of {\em syntactic and semantic contexts}; and
%4) to evaluate {\tool}'s accuracy in code~migration and code synthesis
%applications.

%$n$-gram LM and SLAMC~\cite{fse13}.



%Some projects are replaced if source code is not available anymore.

%We used the latest stable versions at the time of our experiments. 
%We also replaced some projects with new ones when the source code is
%not available anymore.

%\subsection{Data Collection}


% shows the statistics of our dataset. 

%It consists of 10 projects having more than 11,642 files, with 1.15M
%SLOCs and 8,987K $n$-grams with $n$=4. The last 3 columns show the
%sizes of the vocabularies of lexemes, syntaxemes, and sememes.


% Table generated by Excel2LaTeX from sheet 'systems'
\begin{table}[t]
  \centering
  \scriptsize
%footnotesize
%  \tabcolsep 3pt 
  \renewcommand{\arraystretch}{0.9}
  \caption{Subject Projects}
    \begin{tabular}{l|l|r|r|r|r|r|r}
    \hline
    Project & Rev & Files & KSLOCs & $n$-grams & $V_{lex}$   & $V_{syn}$  & $V_{sem}$ \\
    \hline
    ant   & 1.9.4 & 1,233 & 112.4 &        830,152  &    15,899  & 78    &      1,260  \\
    antlr & 3.5.1 & 276   & 40.3  &        264,640  &      5,534  & 77    &         538  \\
    batik & 1.7   & 1,447 & 152.8 &    1,174,800  &    21,709  & 76    &      1,590  \\
    cassandra & 2.1.2 & 960   & 190.9 &    1,450,201  &    18,601  & 78    &      1,330  \\
    db4o  & 7.2   & 1,722 & 83.6  &        620,229  &    10,381  & 75    &      1,249  \\
    itext & 5.3.5 & 503   & 69.3  &        612,571  &    11,648  & 77    &      1,158  \\
    jgit  & 2.3.0 & 1,011 & 101.8 &        858,799  &    13,494  & 78    &      1,295  \\
    lucene & 2.4.0 & 958   & 102.6 &        815,002  &    10,823  & 78    &      1,341  \\
    maven & 3.2.5 & 905   & 63.9  &        434,538  &      7,571  & 77    &      1,095  \\
    poi   & 3.8   & 2627  & 231.0 &    1,926,035  &    34,747  & 78    &      2,164  \\

    \hline
    \end{tabular}%
  \label{systemtab}%
\end{table}%



%\subsection{Featre Extraction}

%\input{features}

\subsection{Data Collection, Experimental Setting and Metrics}

\input{saner18-features}

%We implemented a tool which parse all source files from project to
%extract syntaxemes, sememes and lexemes. Given the lexemes of each
%source file, we evaluate the prediction of the lexeme at each
%location, given $n-1$ previous lexemes and the syntactic and semantic
%context.

%To evaluate our approach's accuracy and to compare it with other approach, we use the metrics of $top-k$ accuracy .....

\input{saner18-eval-different-contexts}

\input{saner18-comparison}

\input{saner18-sensitivity}

%\input{bayesmodel}

\input{msr17-app}

%\input{codesynthesis}

\input{msr17-time}

\input{msr17-casestudies}

\input{msr17-threats}
